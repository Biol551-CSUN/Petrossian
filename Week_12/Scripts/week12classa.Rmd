---
title: "Week 12 Class A"
author: "Cynthia Petrossian"
date: "4/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Libraries
```{r}
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
```

# Manipulation
```{r}
paste("High temp", "Low pH") #space deault separation, brings strings together
paste("High temp", "Low pH", sep = "-") #set separation
paste0("High temp", "Low pH") #no space

shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)

two_cities <- c("best", "Worst")
paste("It was the", two_cities, "of times.")

shapes #vector of shapes
str_length(shapes)
seq_data <- c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4)
str_sub(seq_data, start = 3, end = 3) <- "A"
seq_data
str_dup(seq_data, times = c(2,3))
```
# Whitespace
```{r}
badtreatments <- c("High", " High", "High ", "Low", "Low")
badtreatments
str_trim(badtreatments) #removes white space from both sides
str_trim(badtreatments, side = "left") #removes from left
str_pad(badtreatments, 5, side = "right") #add a white space to the right to make them all 5 characters long 

```

# Locale Sensitive
```{r}
x <- "I Love R!"
str_to_upper(x)
str_to_lower(x)
str_to_title(x)
```

# Pattern Matching
```{r}
data <- c("AAA", "TATA", "CTAG", "GCTT")
str_view(data, pattern = "A")
str_detect(data, pattern = "A")
str_detect(data, pattern = "AT")
str_locate(data, pattern = "AT")
```

# Regex
```{r}
vals <- c("a.b", "b.c", "c.d")
str_replace(vals, "\\.", " ")


val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")
str_count(val2, "[aeiou]")
str_count(val2, "[0-9]")
```

# Find the Phone Numbers
```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
phone <- "([2-9][0-9]{2})[-.]([0-9]{3})[-.]([0-9]{4})"
str_detect(strings, phone)
test <- str_subset(strings, phone)
test

str_replace_all(test, "\\.", "\\-") %>% 
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% 
  str_trim()
```

# tidytext
```{r}
head(austen_books())

original_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(line = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup()
head(original_books)

tidy_books <- original_books %>% 
  unnest_tokens(output = word, input = text)
head(tidy_books)

head(get_stopwords())
cleaned_books <- tidy_books %>% 
  anti_join(get_stopwords())
head(cleaned_books)

cleaned_books %>% 
  count(word, sort = TRUE)

sent_word_counts <- tidy_books %>% 
  inner_join(get_sentiments()) %>% 
  count(word, sentiment, sort = TRUE)

sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

# Make a Wordcloud
```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3)
```

